{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c42cd7",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch and binary classification\n",
    "\n",
    "## First, a few words on the goal of this tutorial\n",
    "\n",
    "With this and all other tutorials this week, there's a need to strike a balance between two somewhat competing goals:\n",
    "1. ML in theory: what is ML? how does it work? etc. \n",
    "2. ML in practice: what tools are there? how do I use them? what are the tips and tricks of the trade for doing ML on HEP data sets?\n",
    "\n",
    "On one hand nothing will make much sense if we totally skip over #1. On the other, #2 is really what you're here to know. Broadly speaking this tutorial will spend a bit of time dwelling on #1, and #2 will be the focus of the afternoon tutorial, but please let us know if we move past the fundamentals too quickly!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239b726",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Machine learning packages, in particular PyTorch\n",
    "- Tensors and tensor operations\n",
    "\n",
    "2. Gradients and optimizers\n",
    "- Some autodiff basics\n",
    "- Toy dataset construction\n",
    "- Optimizer for linear regression\n",
    "\n",
    "3. Deep learning\n",
    "- Building a multi-layer perceptron\n",
    "- Training loops\n",
    "- Binary classification metrics: ROC curves, background rejection, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00852ba",
   "metadata": {},
   "source": [
    "## Machine learning packages, in particular PyTorch\n",
    "\n",
    "Broadly speaking, deep learning frameworks are software packages that support:\n",
    "1. Tensor operations of the type that make up deep neural networks\n",
    "2. Calculation of gradients\n",
    "3. Optimization of model parameters given said gradients\n",
    "\n",
    "There are many packages available in many languages. Python is the dominant language, probably followed by Julia and then many others. The following Python packages are the most relevant:\n",
    "- PyTorch: Probably the most widely used for research today, and what we'll be using in these tutorials. Originally developed by Meta AI\n",
    "- Tensorflow: Also frequently used, but less supported than it used to be. Originally developed by Google brain\n",
    "- JAX: Broader library for more than just training deep neural networks (see differentiable programming tutorials tomorrow), but also can be used like PyTorch or Tensorflow. Under development by Google, in some sense is Tensorflow v3.\n",
    "- Scikit Learn: Broad library of machine learning models (not just neural networks!) and utility functions. Very frequently used in combination with the other frameworks\n",
    "\n",
    "Others include Apache MXNet, Keras (now integrated into Tensorflow), Theano (old school), TMVA (don't use!). You might run into these if looking at older codebases, but if starting out from scratch, you're best advised to use one of the above.\n",
    "\n",
    "For this tutorial we'll be using PyTorch, but everything in this tutorial could also be achieved using any of the packages above.\n",
    "As an added tip, if you find code that uses one package but prefer another, LLMs are highly accurate at translating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fundamental unit of data in PyTorch is the tensor. In many ways, they work just like numpy arrays\n",
    "example_numpy_array = np.array([1, 2, 3, 4, 5])\n",
    "example_tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(f\"Example numpy array: {example_numpy_array}\")\n",
    "print(f\"Example tensor: {example_tensor}\")\n",
    "\n",
    "# Tensors can be used in many of the same ways as numpy arrays\n",
    "print(f\"Example numpy array + 1: {example_numpy_array + 1}\")\n",
    "print(f\"Example tensor + 1: {example_tensor + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More relevant to training deep neural networks, we can also do matrix multiplication\n",
    "m1 = np.random.randn(3,4)\n",
    "m2 = np.random.randn(4,3)\n",
    "print(f\"m1: {m1}\")\n",
    "print(f\"m2: {m2}\")\n",
    "\n",
    "# The @ sign is a nice shorthand for matrix multiplication\n",
    "m3 = m1 @ m2\n",
    "print(f\"m3: {m3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same operations work in pytorch. Note we can build torch tensors directly from numpy arrays\n",
    "m1_tensor = torch.tensor(m1)\n",
    "m2_tensor = torch.tensor(m2)\n",
    "\n",
    "# The @ sign works in pytorch as well\n",
    "m3_tensor = m1_tensor @ m2_tensor\n",
    "print(f\"m3_tensor: {m3_tensor}\")\n",
    "\n",
    "# Can also use the torch.matmul function\n",
    "m4_tensor = torch.matmul(m1_tensor, m2_tensor)\n",
    "print(f\"m4_tensor: {m4_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a87efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also build tensors directly using PyTorch, and do essentially any operation that might be supported for numpy arrays.\n",
    "m5_tensor = torch.linspace(0, 1, 10)\n",
    "\n",
    "# We can add dimensions to tensors using the \"unsqueeze\" method\n",
    "# Note the argument \"axis\" typical in numpy becomes \"dim\" in PyTorch\n",
    "m6_tensor = m5_tensor.unsqueeze(dim=1)\n",
    "print(f\"m6_tensor: {m6_tensor}\")\n",
    "print(f\"m6_tensor.shape: {m6_tensor.shape}\\n\")\n",
    "\n",
    "# We can also use the \"view\" method to reshape tensors\n",
    "m7_tensor = m6_tensor.view(2, 5)\n",
    "print(f\"m7_tensor: {m7_tensor}\")\n",
    "print(f\"m7_tensor.shape: {m7_tensor.shape}\\n\")\n",
    "\n",
    "# Or the expand (returns a view) or repeat (returns a copy) method to repeat the data within a tensor\n",
    "m8_tensor = m6_tensor.expand(-1, 10)\n",
    "print(f\"m8_tensor: {m8_tensor}\")\n",
    "print(f\"m8_tensor.shape: {m8_tensor.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"cat\" method concatenates tensors along a given dimension\n",
    "## YOUR CODE HERE: concatenate m5_tensor with itself along the first dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec5186",
   "metadata": {},
   "source": [
    "## Gradients and optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1dbdbc",
   "metadata": {},
   "source": [
    "Fast operations are great, but the most important feature of PyTorch is the ability to compute gradients.\n",
    "They are ultimately the thing you need in order to train a neural network to do anything useful.\n",
    "In the vast majority of applications within HEP, you can forget about these details and just let the code compute gradients for you.\n",
    "However, it's useful to see some examples of computing gradients at least once.\n",
    "To start, we'll need some input tensors, and a function to compute the gradients of.\n",
    "Let's use as an example a simple 1D function:\n",
    "\n",
    "$$y = A x^2 + b$$\n",
    "\n",
    "Let's compute the gradient of $y$ with respect to $A$, $x$ and $b$. To tell PyTorch we'd like it to compute a gradient, we need to set `requires_grad=True` when building the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c61d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we also need to set the data type to float32, you can't take a gradient of an integer tensor!\n",
    "A = torch.tensor([2], requires_grad=True, dtype=torch.float32)\n",
    "b = torch.tensor([3], requires_grad=True, dtype=torch.float32)\n",
    "x = torch.tensor([5], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "y = A @ x**2 + b\n",
    "\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "# Now let's compute the gradients of y with respect to A, b, and x\n",
    "y.backward()\n",
    "\n",
    "print(f\"dy/dA (x**2): {A.grad}\")\n",
    "print(f\"dy/db (1): {b.grad}\")\n",
    "print(f\"dy/dx (2*A*x): {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df98d22",
   "metadata": {},
   "source": [
    "Just like calculus class! Underneath the hood, pytorch is constructing a computational graph that represents the function you are trying to compute. In what's called the \"forward pass\", PyTorch computes your function and keeps track of the derivative of each operation with respect to it's inputs. Then when you call `y.backward()`, PyTorch uses the chain rule to evaluate the gradient of each part of the graph that has `requires_grad=True` with respect to the scalar value. Let's try to break this by giving PyTorch a function we know is non-differentiable, like $y = A|x|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aecad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At x = 5, the function is differentiable and everything works fine\n",
    "A = torch.tensor([2], requires_grad=True, dtype=torch.float32)\n",
    "x = torch.tensor([5], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "y = A * torch.abs(x)\n",
    "\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(f\"dy/dA (x): {A.grad}\")\n",
    "print(f\"dy/dx (A): {x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66048362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about at x = 0?\n",
    "A = torch.tensor([2], requires_grad=True, dtype=torch.float32)\n",
    "x = torch.tensor([0], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "y = A * torch.abs(x)\n",
    "\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "## YOUR CODE HERE: calculate the gradients of y with respect to A and x\n",
    "\n",
    "print(f\"dy/dA (x): {A.grad}\")\n",
    "print(f\"dy/dx (A): {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549bd57",
   "metadata": {},
   "source": [
    "What happened? The derivative is undefined, so you might have expected to see a NaN gradient, but instead we got 0. This is an autodiff at work! For details, see https://docs.pytorch.org/docs/stable/notes/autograd.html, but long-story short is that PyTorch has tricks for catching undefined gradients and turning them into whatever is most likely to be useful for performing the optimizations users are interested in. \n",
    "\n",
    "As an optional exercise, trying taking gradients of other pathological functions and see what PyTorch returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec05d1ca",
   "metadata": {},
   "source": [
    "To do something useful with these gradients, we'll need to get a bit more advanced and build a toy dataset that we can use for a binary classification problem.\n",
    "We'll do this using the `torch.distributions` package, which let's you model many different kinds of probability distributions with pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b495e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a toy dataset that contains samples from two overlapping 2D Gaussian distributions\n",
    "# We will call the Gaussian with mean 0 and variance 1 background, and a Gaussian with mean 2 and variance 0.5 signal\n",
    "# Note we have a diagonal covariance matrix, so the two dimensions are uncorrelated. Make this more complicated if you want to!\n",
    "\n",
    "bkg_dist = torch.distributions.MultivariateNormal(loc=torch.tensor([0.0, 0.0]), covariance_matrix=torch.tensor([[1.0, 0.0], [0.0, 1.0]]))\n",
    "sig_dist = torch.distributions.MultivariateNormal(loc=torch.tensor([1.5, 1.5]), covariance_matrix=torch.tensor([[0.5, 0.0], [0.0, 0.5]]))\n",
    "\n",
    "# input_bkg is 500 samples from a gaussian of N(0,1)\n",
    "N_bkg = 1000\n",
    "input_bkg = bkg_dist.sample((N_bkg,))\n",
    "\n",
    "# input_sig is 500 samples from a gaussian of N(1.5,0.5)\n",
    "N_sig = 300\n",
    "input_sig = sig_dist.sample((N_sig,))\n",
    "\n",
    "# Plot input_x1 and input_x2\n",
    "plt.plot(input_bkg[:, 0], input_bkg[:, 1], '.', alpha=0.5, label=\"input bkg\")\n",
    "plt.plot(input_sig[:, 0], input_sig[:, 1], '.', alpha=0.5, label=\"input sig\")\n",
    "plt.axis('equal')\n",
    "plt.ylabel(r\"$x_1$\")\n",
    "plt.xlabel(r\"$x_2$\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027181ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know what points are samples from which distribution, but in real world examples we don't, so let's pretend and put the data together in a single array.\n",
    "# This data will be used as input to machine learning models in what follows.\n",
    "\n",
    "## YOUR CODE HERE: create a tensor of shape (N_bkg + N_sig, 2) that contains the background and signal samples together\n",
    "\n",
    "# Plot input_x\n",
    "\n",
    "plt.plot(input_x[:, 0], input_x[:, 1], '.', c=\"green\", alpha=0.5, label=\"input x\")\n",
    "plt.axis('equal')\n",
    "plt.ylabel(r\"$x_1$\")\n",
    "plt.xlabel(r\"$x_2$\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ccd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In supervised learning, we have a dataset with class labels that we can use to calculate a loss function and train the model.\n",
    "# Let's build these labels.\n",
    "\n",
    "label_bkg = torch.zeros(input_bkg.shape[0])  # Assign label 0 to input_x1\n",
    "label_sig = torch.ones(input_sig.shape[0])  # Assign label 1 to input_x2\n",
    "\n",
    "input_y = torch.cat([label_bkg, label_sig], dim=0).unsqueeze(-1)\n",
    "\n",
    "# Plot input_y on the z-axis\n",
    "\n",
    "sc = plt.scatter(input_x[:, 0], input_x[:, 1], c=input_y.flatten(), cmap='viridis', alpha=0.5)\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "cbar = plt.colorbar(sc, ticks=[0, 1])  # Set the colorbar ticks to only show 0 and 1\n",
    "cbar.set_label('input y (binary lables)')\n",
    "plt.ylabel(r\"$x_1$\")\n",
    "plt.xlabel(r\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3801c",
   "metadata": {},
   "source": [
    "Now we have some data.\n",
    "Let's see what we can do with PyTorch's gradients, first starting with a very simple linear model.\n",
    "Let's define $y = \\sigma(Ax + b)$, where $x$ will be a sample from our 2D data distribution, $A$ and $b$ will be the parameters of our model, $\\sigma$ represents the sigmoid function, and $y$ will be interpreted as a probability that the data sample $x$ belongs to the signal class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a88dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of our model\n",
    "A = torch.randn(2, 1, requires_grad=True, dtype=torch.float32)  # We'll initialize our parameters to samples from a normal distribution\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "# Define the model\n",
    "def model(x):\n",
    "    return sigmoid(x @ A + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform a forward pass through our model\n",
    "out = model(input_x[0,:])\n",
    "print(f\"out: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567204c0",
   "metadata": {},
   "source": [
    "The next thing we need is a loss function. \n",
    "We'll use the binary cross entropy loss, which is standard in binary classification tasks:\n",
    "\n",
    "$$\n",
    "L[f] = \\frac{1}{N} \\sum_{i}^N y_i \\log(f(x_i)) + (1 - y_i) \\log(1 - f(x_i))\n",
    "$$\n",
    "\n",
    "where $f$ is the model, $x_i$ is the data, and $y_i$ is a class label (0 for background, 1 for signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17522b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_pred, y_true):\n",
    "    return -torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34902bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the loss\n",
    "loss = loss_fn(out, input_y[0])\n",
    "print(f\"label: {input_y[0]}\")\n",
    "print(f\"prediction: {out}\")\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now compute the gradient of the loss with respect to the parameters of our model\n",
    "loss.backward()\n",
    "\n",
    "print(f\"dL/dA: {A.grad}\")\n",
    "print(f\"dL/db: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1a8cc",
   "metadata": {},
   "source": [
    "The last piece is to optimize our model parameters to solve the binary classification task.\n",
    "In realistic tasks it is always best to use off the shelf optimizers which contain many, many tricks to get good convergence in difficult optimization problems.\n",
    "However this first example is so simple that we can optimize the parameters ourselves in a few lines of code, and write our very first training loop along the way.\n",
    "We will implement very simple gradient descent, with no batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d43195",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "max_grad = 10\n",
    "losses = []\n",
    "\n",
    "# We'll run a maximum of 1000 iterations\n",
    "for i in range(5001):\n",
    "\n",
    "    # Forward pass\n",
    "    out = model(input_x)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(out, input_y)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    # In a realistic case, we would replace these lines with a call to a real optimizer\n",
    "    # Note I am clipping the gradients here to avoid large gradients that can cause numerical instability\n",
    "    # What happens if you don't do this?\n",
    "    A.data -= lr * A.grad.clip(min=-max_grad, max=max_grad)\n",
    "    b.data -= lr * b.grad.clip(min=-max_grad, max=max_grad)\n",
    "\n",
    "    if loss < 0.1:\n",
    "        print(f\"Loss is below 0.1, stopping training\")\n",
    "        break\n",
    "\n",
    "print(f\"Ran for {i} iterations\")\n",
    "print(f\"Final loss: {loss.item()}\")\n",
    "print(f\"Final A: {A.detach().numpy()}\")\n",
    "print(f\"Final b: {b.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e16439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the model output as a function of the 2D input space to see if we learned something useful\n",
    "x1 = np.linspace(input_x[:,0].min(), input_x[:,0].max(), 100)\n",
    "x2 = np.linspace(input_x[:,1].min(), input_x[:,1].max(), 100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "X = np.vstack([X1.flatten(), X2.flatten()]).T\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = model(X).detach().numpy()\n",
    "\n",
    "ctr = plt.contour(X1, X2, Y.reshape(100, 100), levels=15, cmap='viridis')\n",
    "sc = plt.scatter(input_x[:, 0], input_x[:, 1], c=input_y.flatten(), cmap='viridis', alpha=0.5)\n",
    "cbar = plt.colorbar(ctr)\n",
    "cbar.set_label('Model output')\n",
    "plt.ylabel(r\"$x_1$\")\n",
    "plt.xlabel(r\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also calculate a ROC curve and the area under to get a performance metric\n",
    "# Get the model output for the entire data set\n",
    "y_pred_linear = model(input_x).detach().numpy()\n",
    "\n",
    "# Get the ROC curve\n",
    "fpr_linear, tpr_linear, thresholds_linear = sklearn.metrics.roc_curve(input_y, y_pred_linear)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr_linear, tpr_linear)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()\n",
    "\n",
    "# Get the area under the ROC curve\n",
    "auc = sklearn.metrics.auc(fpr_linear, tpr_linear)\n",
    "\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2a7c3",
   "metadata": {},
   "source": [
    "Remember AUC ranges between 0.5 (random guessing) and 1.0 so this isn't too bad at all.\n",
    "This is a linear model, so we should obviously expect that our network output should be linear, i.e. the contours of the 2D functions are straight lines.\n",
    "Note I had to fiddle a bit with the parameters above to get reliably decent results.\n",
    "It should be not at all hard to break, which you should try!\n",
    "\n",
    "So that's a simple linear model, but on real world tasks this won't get you very far.\n",
    "Let's move on and solve this toy problem for good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd362136",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "For all things deep learning, PyTorch has a `torch.nn` module that contains nearly all of the building blocks you might need implemented for you.\n",
    "Fundamental to the module is the `torch.nn.Module` class, which is the base class for all neural network pieces, and neural networks as a whole.\n",
    "Whenever you start to write a neural network from scratch in PyTorch, the first thing to do is subclass `torch.nn.Module` and implement two methods:\n",
    "- `__init__`: The constructor method. Here you'll initialize all of the network layers\n",
    "- `forward`: Define the forward pass of the network. This function should take an input tensor as an argument, which is the data of the mini-batch, pass the tensor through the layers of the network, and then return the network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7501c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifierNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network with two hidden layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # The constructor method, initialize the network layers\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()  # Always be sure to call the parent constructor!\n",
    "        self.linear_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # The forward pass of the network\n",
    "    def forward(self, x):\n",
    "        return self.linear_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cef642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network and print a summary using torchinfo\n",
    "model = BinaryClassifierNN(input_dim=2, hidden_dim=50)\n",
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e835cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that we can run a forward pass with our model\n",
    "## YOUR CODE HERE: run a forward pass through the model, just to check that we can get outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b4fa9",
   "metadata": {},
   "source": [
    "The next improvement is to use a proper optimizer.\n",
    "There are a whole host of them implemented in the package `torch.optim` with even more implemented by someone else on github.\n",
    "A very common starting point is `Adam`.\n",
    "If you're training something big and need efficiency or to squeeze out every last bit of performance there are better options, but Adam will work great here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b5db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "# Re-initializing the model here so you don't end up with an optimizer for a different model that's now out of scope (tricky in jupyter notebooks)\n",
    "model = BinaryClassifierNN(input_dim=2, hidden_dim=50)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397fa2e",
   "metadata": {},
   "source": [
    "Now for the training loop!\n",
    "It is conceptually identical to what we did above.\n",
    "We just rely on the optimizer instead of adjusting the parameters of the model manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eebd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "max_epochs = 1000\n",
    "losses = []\n",
    "\n",
    "# Epoch loop\n",
    "for epoch in tqdm.tqdm(range(max_epochs)):\n",
    "\n",
    "    # Zero the gradients from the previous batch. Don't forget this!\n",
    "    optim.zero_grad()\n",
    "\n",
    "    ## YOUR CODE HERE: implement the forward and backward pass needed to train the model\n",
    "\n",
    "    # 1. Forward pass\n",
    "\n",
    "    # 2. Compute the loss\n",
    "\n",
    "    # 3. Backward pass\n",
    "\n",
    "    # Update the model parameters\n",
    "    optim.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5de110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the contours of the NN output\n",
    "x1 = np.linspace(input_x[:,0].min(), input_x[:,0].max(), 100)\n",
    "x2 = np.linspace(input_x[:,1].min(), input_x[:,1].max(), 100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "X = np.vstack([X1.flatten(), X2.flatten()]).T\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = model(X).detach().numpy()\n",
    "\n",
    "ctr = plt.contour(X1, X2, Y.reshape(100, 100), levels=15, cmap='viridis')\n",
    "sc = plt.scatter(input_x[:, 0], input_x[:, 1], c=input_y.flatten(), cmap='viridis', alpha=0.5)\n",
    "cbar = plt.colorbar(ctr)\n",
    "cbar.set_label('Model output')\n",
    "plt.ylabel(r\"$x_1$\")\n",
    "plt.xlabel(r\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09fc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also calculate a ROC curve and the area under to get a performance metric\n",
    "# Get the model output for the entire data set\n",
    "## YOUR CODE HERE: get the model output for the entire data set\n",
    "## y_pred_nn = ...\n",
    "\n",
    "# Get the ROC curve\n",
    "fpr_nn, tpr_nn, thresholds_nn = sklearn.metrics.roc_curve(input_y, y_pred_nn)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr_nn, tpr_nn)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()\n",
    "\n",
    "# Get the area under the ROC curve\n",
    "auc_nn = sklearn.metrics.auc(fpr_nn, tpr_nn)\n",
    "\n",
    "print(f\"AUC: {auc_nn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36ac0e",
   "metadata": {},
   "source": [
    "Congrats!\n",
    "You've trained for first deep learning model. \n",
    "You should hopefully see that the network has learned a non-linear solution, and that your separation power, quantified by the AUC, is a bit higher.\n",
    "One a toy dataset like this the results aren't so impressive, but as you'll see the power of this technology is in scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852a089",
   "metadata": {},
   "source": [
    "## Bonus: an analytic solution and a sad truth\n",
    "\n",
    "There is one last lesson we can learn from this toy problem.\n",
    "It is actually simple enough that it has an analytic solution.\n",
    "What this solution is and how to calculate it is a bit beyond the scope here (you're almost always just going to learn these things from data anyway!), but very briefly the correct solution is a rescaling of the likelihood ratio between the signal and background distributions, both of which we know are simple 2D Gaussians.\n",
    "We can use the `torch.distributions` package to evaluate the likelihoods numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca13303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for calculating the analytic solution at a given point\n",
    "def analytic_solution(x):\n",
    "    \"\"\" not the input x is a 1D torch tensor with shape (2,) \"\"\"\n",
    "    llr = sig_dist.log_prob(x) - bkg_dist.log_prob(x)\n",
    "    return torch.exp(llr) / (1 + torch.exp(llr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef98a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What ROC curve and AUC do we get for the analytic solution?\n",
    "analytic_pred = analytic_solution(input_x)\n",
    "\n",
    "# Calculate the ROC curve\n",
    "fpr_analytic, tpr_analytic, thresholds_analytic = sklearn.metrics.roc_curve(input_y, analytic_pred)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc_analytic = sklearn.metrics.auc(fpr_analytic, tpr_analytic)\n",
    "print(f\"AUC model: {auc_nn}\")\n",
    "print(f\"AUC analytic: {auc_analytic}\")\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr_nn, tpr_nn, label=f\"Model: AUC = {auc_nn:.4f}\")\n",
    "plt.plot(fpr_analytic, tpr_analytic, label=f\"Analytic: AUC = {auc_analytic:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de4b9e0",
   "metadata": {},
   "source": [
    "The NN is likely getting a slightly higher AUC than the analytic solution. The problem is that the NN is only getting a better AUC on the dataset we explicitly trained it on. What if we sample a larger data set and try again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70851f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try resampling the data\n",
    "N_bkg = 100000\n",
    "N_sig = 30000\n",
    "input_bkg = bkg_dist.sample((N_bkg,))\n",
    "input_sig = sig_dist.sample((N_sig,))\n",
    "input_x = torch.cat([input_bkg, input_sig], dim=0)\n",
    "input_y = torch.cat([torch.zeros(N_bkg), torch.ones(N_sig)], dim=0)\n",
    "analytic_pred = analytic_solution(input_x)\n",
    "nn_pred = model(input_x).detach().numpy()\n",
    "\n",
    "# Calculate the ROC curve\n",
    "fpr_analytic, tpr_analytic, thresholds_analytic = sklearn.metrics.roc_curve(input_y, analytic_pred)\n",
    "fpr_nn, tpr_nn, thresholds_nn = sklearn.metrics.roc_curve(input_y, nn_pred)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc_analytic = sklearn.metrics.auc(fpr_analytic, tpr_analytic)\n",
    "auc_nn = sklearn.metrics.auc(fpr_nn, tpr_nn)\n",
    "print(f\"AUC model: {auc_nn:.4f}\")\n",
    "print(f\"AUC analytic: {auc_analytic:.4f}\")\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr_nn, tpr_nn, label=f\"Model: AUC = {auc_nn:.4f}\")\n",
    "plt.plot(fpr_analytic, tpr_analytic, label=f\"Analytic: AUC = {auc_analytic:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202764b2",
   "metadata": {},
   "source": [
    "Now you should see the the model's AUC is a bit lower than the analytic solution.\n",
    "We're starting to see \"over-training\", where the model starts to memorize your dataset, and we'll cover how to deal with it in the next lecture.\n",
    "So the analytic solution seems to work.\n",
    "Let's plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the analytic solution, in an expanded range to see the full shape\n",
    "x1 = np.linspace(-3, 6, 200)\n",
    "x2 = np.linspace(-3, 6, 200)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "X = np.vstack([X1.flatten(), X2.flatten()]).T\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = analytic_solution(X).detach().numpy()\n",
    "\n",
    "ctr = plt.contour(X1, X2, Y.reshape(200, 200), levels=15, cmap='viridis')\n",
    "sc = plt.scatter(input_x[:, 0], input_x[:, 1], c=input_y.flatten(), cmap='viridis', alpha=0.5)\n",
    "cbar = plt.colorbar(ctr)\n",
    "cbar.set_label('Model output')\n",
    "plt.ylabel(r\"$x_1$\")\n",
    "plt.xlabel(r\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be9f3d",
   "metadata": {},
   "source": [
    "This is not very similar to what the NN learned!\n",
    "Unsurprisingly the NN only learned the solution in the region of phase space where there is training data.\n",
    "Outside of this region, the ML based solution is entirely unhelpful.\n",
    "This is just an elaborate way to make the obvious statement that ML can only help you where you have data.\n",
    "In most cases we can ignore this detail and everything will be fine, but it is good to recognize the fundamental limitations of the approach before we add more layers of complexity, which we will do next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lbl-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
