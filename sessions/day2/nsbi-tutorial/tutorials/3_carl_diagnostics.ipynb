{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fe27ba",
   "metadata": {},
   "source": [
    "# NSBI diagnostics\n",
    "\n",
    "In the previous chapter, we've trained CARL models that approximate the density ratio between two hypotheses of an event:\n",
    "\n",
    "$$\n",
    " \\hat{r}(x ; H_1, H_2) \\sim r ( x ; H_1, H_2) \\equiv \\frac{p(x | H_1)}{p(x | H_2)}\n",
    "$$\n",
    "\n",
    "The validity of our NSBI results, of course, depends on close this approixmation, $~$, holds. We will perform two diagnostics to gauge how well our estimates are performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529af8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "import vector\n",
    "import hist\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightning as L\n",
    "\n",
    "from physics.analysis import zz4l, zz2l2v\n",
    "from datasets.balanced import BalancedDataset\n",
    "from nsbi import carl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8baf229",
   "metadata": {},
   "source": [
    "## 0. Load the training/validation data, scaler, and model checkpoint\n",
    "\n",
    "First, we'll load everything we need. Note that if you set the same seed for the two models come into play, you only need to load denominator hypothesis datasets once; otherwise you will need the denominator datasets corresponding to each model separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "917aeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = 'run/h4l'\n",
    "\n",
    "(events_sig_train, events_sig_val), (events_bkg_train, events_bkg_val) = carl.utils.load_data(run_dir, 'sig_over_bkg')\n",
    "(events_sbi_train, events_sbi_val), _ = carl.utils.load_data(run_dir, 'sbi_over_bkg')  # same seed!\n",
    "\n",
    "scaler_sig_over_bkg, model_sig_over_bkg = carl.utils.load_results(run_dir, 'sig_over_bkg')\n",
    "scaler_sbi_over_bkg, model_sbi_over_bkg = carl.utils.load_results(run_dir, 'sbi_over_bkg')\n",
    "\n",
    "features_4l = ['l1_pt', 'l1_eta', 'l1_phi', 'l1_energy', 'l2_pt', 'l2_eta', 'l2_phi', 'l2_energy', 'l3_pt', 'l3_eta', 'l3_phi', 'l3_energy', 'l4_pt', 'l4_eta', 'l4_phi', 'l4_energy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331bfd60",
   "metadata": {},
   "source": [
    "## 1. Reweighting\n",
    "\n",
    "An intuitive expectation of a DRE is that it should provide a correspondingly accurate reweighting of an event between hypotheses:\n",
    "\n",
    "$$\n",
    " \\hat{p}(x | H_1) = p(x| H_2) \\times \\hat{r}(x ; H_1, H_2) \\Rightarrow \\hat{w}_{H_1}(x) = w_{H_2}(x) \\times r( x ; H_1, H_2) / N\n",
    "$$\n",
    "\n",
    "where $N$ is an arbitrary normalization factor (remember: we are performing a *density* ratio estimate!). In order to perform this check for one estimate, you must:\n",
    "\n",
    "1. Scale the features of the denominator hypothesis events using the scaler from training.\n",
    "2. Run the model over the scaled features.\n",
    "3. Perform the likelihood trick over the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2a9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT ME\n",
    "events_num_train, events_num_val = events_sbi_train, events_sbi_val \n",
    "scaler = scaler_sbi_over_bkg\n",
    "model = model_sbi_over_bkg\n",
    "events_denom_train, events_denom_val = events_bkg_train, events_bkg_val\n",
    "\n",
    "X_denom_train, X_denom_val = scaler.transform(events_denom_train.kinematics[features_4l].to_numpy()), scaler.transform(events_denom_val.kinematics[features_4l].to_numpy())\n",
    "dl_denom_train, dl_denom_val = DataLoader(TensorDataset(torch.tensor(X_denom_train, dtype=torch.float32)), batch_size=1024), DataLoader(TensorDataset(torch.tensor(X_denom_val, dtype=torch.float32)), batch_size=1024) \n",
    "\n",
    "trainer = L.Trainer(accelerator='gpu', devices=1)\n",
    "s_num_vs_denom = torch.cat(trainer.predict(model, dl_denom_train))\n",
    "r_num_over_denom = s_num_vs_denom / (1 - s_num_vs_denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4b68f",
   "metadata": {},
   "source": [
    "# 1. (b) Compare numerator vs. denominator-reweighted distributions\n",
    "\n",
    "1. Multiply the DRE onto the denominator hypothesis event weights.\n",
    "2. Compare distributions of an observable, e.g. $m_{4\\ell}$, obtained using (1) numerator hypothesis events & weights, and (2) denominator hypothesis events & reweights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad9134d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4l_axis = hist.axis.Regular(21, 180, 1000, label = 'm4l')\n",
    "h_m4l_num = hist.Hist(m4l_axis)\n",
    "h_m4l_num_from_denom = hist.Hist(m4l_axis)\n",
    "\n",
    "def calculate_m4l(kinematics):\n",
    "    p_l1 = vector.array({'pt': kinematics['l1_pt'], 'eta': kinematics['l1_eta'], 'phi': kinematics['l1_phi'], 'energy': kinematics['l1_energy']})\n",
    "    p_l2 = vector.array({'pt': kinematics['l2_pt'], 'eta': kinematics['l2_eta'], 'phi': kinematics['l2_phi'], 'energy': kinematics['l2_energy']})\n",
    "    p_l3 = vector.array({'pt': kinematics['l3_pt'], 'eta': kinematics['l3_eta'], 'phi': kinematics['l3_phi'], 'energy': kinematics['l3_energy']})\n",
    "    p_l4 = vector.array({'pt': kinematics['l4_pt'], 'eta': kinematics['l4_eta'], 'phi': kinematics['l4_phi'], 'energy': kinematics['l4_energy']})\n",
    "    return (p_l1 + p_l2 + p_l3 + p_l4).mass\n",
    "\n",
    "\n",
    "# IMPLEMENT ME\n",
    "# NOTE: What should you do about the arbitrary normalization factor, N, from above?\n",
    "w_num = torch.tensor(events_num_train.weights)\n",
    "w_num_from_denom = torch.tensor(events_denom_train.weights) * r_num_over_denom\n",
    "w_num /= torch.sum(w_num)\n",
    "w_num_from_denom /= torch.sum(w_num_from_denom)\n",
    "\n",
    "h_m4l_num.fill( calculate_m4l(events_num_train.kinematics), weight=w_num)\n",
    "h_m4l_num_from_denom.fill( calculate_m4l(events_denom_train.kinematics), weight=w_num_from_denom)\n",
    "\n",
    "h_m4l_num.plot()\n",
    "h_m4l_num_from_denom.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab87a64",
   "metadata": {},
   "source": [
    "As performed in Chapter 1, you are encouraged to check in more detail via ratio plots. Qualitatively, are the histograms \"compatible\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5b149",
   "metadata": {},
   "source": [
    "## 2. Calibration curve\n",
    "\n",
    "A more direct and systematic way to check is to obtain what is referred to as the _calibration_ curve of the DRE.\n",
    "Directly working with the predictions pre-likelihood ratio trick, the outputs should approximate\n",
    "\n",
    "$$\n",
    "s (x ; H_1, H_2) = \\frac{p_{H_1}(x)}{p_{H_1}(x) + p_{H_2}(x)}\n",
    "$$\n",
    "\n",
    "as an unbiased function of $x$. We can check if this is satisfied by performing the following:\n",
    "\n",
    "1. Bin events sampled from the numerator & denominator hypotheses according the classifier estimate, $\\hat s$. \n",
    "2. In each bin, count the occurrence of events originating from the numerator & denominator hypothesis.\n",
    "    - IMPORTANT: You must count using the balanced weights of each hypothesis!\n",
    "3. Compute an \"MC\" estimate of the decision function in each bin, $s = \\frac{N(y=1)}{N(y=0) + N(y=1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee8411",
   "metadata": {},
   "source": [
    "### 2.(a) Run the model over the numerator & denominator hypothesis datasets\n",
    "\n",
    "\n",
    "We will make use of the pre-available `BalancedDataset` implementation (which was also used in the training) to consistently perform the $x$-scaling, event weight balancing, and $y = 0,1$ labeling of the numerator & denominator hypotheses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92a9059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_balanced = BalancedDataset(events_num_train, events_denom_train, features=features_4l, scaler=scaler)\n",
    "dl = torch.utils.data.DataLoader(torch.tensor(ds_balanced.X, dtype=torch.float32), batch_size=1024)\n",
    "s_num_vs_denom = torch.cat(trainer.predict(model, dl)).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de6912d",
   "metadata": {},
   "source": [
    "### 2.(b) Define the $\\hat{s}$ binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8643934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_bins = np.linspace(0.0, 1.0, 41)\n",
    "s_centers = (s_bins[:-1] + s_bins[1:])/2\n",
    "s_nbins = len(s_bins) -1\n",
    "s_widths = (s_bins[1:] - s_bins[:-1])/2\n",
    "s_axis = hist.axis.Regular(40,0,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2f74e",
   "metadata": {},
   "source": [
    "### 2.(c) Count the events from each hypothesis in each $\\hat{s}$ bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "608b604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_num, h_denom = hist.Hist(s_axis, storage=hist.storage.Weight()), hist.Hist(s_axis, storage=hist.storage.Weight())\n",
    "\n",
    "# IMPLEMENT ME\n",
    "h_num.fill(s_num_vs_denom[ds_balanced.s==1], weight = ds_balanced.w[ds_balanced.s==1])\n",
    "h_denom.fill(s_num_vs_denom[ds_balanced.s==0], weight = ds_balanced.w[ds_balanced.s==0])\n",
    "# ---\n",
    "\n",
    "N = h_num.values()\n",
    "D = h_denom.values()\n",
    "VN = h_num.variances()\n",
    "VD = h_denom.variances()\n",
    "s_mc_val = N / (N + D)\n",
    "s_mc_err = np.sqrt((D / np.square(N + D))**2 * VN + (N / np.square(N + D))**2 * VD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3027a8",
   "metadata": {},
   "source": [
    "### 2.(d) Obtain the calibration curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29bd511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT ME\n",
    "plt.plot([0,1], [0,1], marker='none', linestyle='--', color='grey')\n",
    "plt.xlabel(\"NSBI estimate, $\\hat{s}(x)$\")\n",
    "plt.ylabel(\"MC estimate, $s(x)$\")\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.errorbar(s_centers, s_mc_val, xerr=s_widths, yerr=s_mc_err, linestyle='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebaca2",
   "metadata": {},
   "source": [
    "How does the calibration curve look? Is it close to a diagonal line?\n",
    "\n",
    "**Task: Obtain the calibration curve diagnostic over valdation data, for the $p_{\\rm SBI}(x) / (p_{\\rm SBI}(x) + p_{\\rm B} (x))$ estimate.**\n",
    "\n",
    "Extra: The calibration curve is useful in the sense that now we can quantitavely evaluate how (in)accurate our network predictions are as a function of its input variables. Mathematically speaking, as long as the curve obtained above is _monotonic_, one can in principle derive a calibration function, $C(x)$,\n",
    "\n",
    "$$\n",
    "C(x) \\times \\left< \\hat{s}(x) \\right> = \\left< s(x) \\right>\n",
    "$$,\n",
    "\n",
    "that gives an un-biased (but not without variance) estimate of the probability ratio. For the next chapter we will assume that this calibration is perfect; the next tutorial on ensembling will allow us to get a handle on additional uncertainties that should be accounted for due to statistical sources including this model mis-specification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
