{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d7d732",
   "metadata": {},
   "source": [
    "# Measurement of the signal strength\n",
    "\n",
    "$$\\mathcal{L} (\\mu | x) = \\prod_{i}^{n} p (x_i | \\mu) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "733fc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import lightning as L\n",
    "\n",
    "from physics.simulation import mcfm\n",
    "from physics.analysis import zz4l, zz2l2v\n",
    "from physics.hstar import sigstr\n",
    "from nsbi import carl\n",
    "\n",
    "import matplotlib, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d959247",
   "metadata": {},
   "source": [
    "# 0. Define the parameter space\n",
    "\n",
    "The first thing to do is to define the parameter space that will define the set of hypotheses we wish to test.\n",
    "For us, this is the signal strength parameter, which we will test between 0 and 4:\n",
    "\n",
    "$$ 0 \\leq \\mu \\leq 4 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3196c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_space = torch.linspace(0.0, 4.0, 401)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de297c75",
   "metadata": {},
   "source": [
    "# 1. Open the \"mystery\" dataset\n",
    "\n",
    "A dataset has been generated according to an unknown (to you) value of $\\mu$ at the LHC luminosity of $300\\,\\mathrm{fb}^{-1}$. Let's open this dataset and read out for each event its (1) observables, and (2) number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc76834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 300.0  # ifb\n",
    "events_obs = pd.read_csv('data/obs/mu_z.csv')\n",
    "\n",
    "features = ['l1_pt', 'l1_eta', 'l1_phi', 'l1_energy', 'l2_pt', 'l2_eta', 'l2_phi', 'l2_energy', 'l3_pt', 'l3_eta', 'l3_phi', 'l3_energy', 'l4_pt', 'l4_eta', 'l4_phi', 'l4_energy']\n",
    "events_obs_features = events_obs[features]\n",
    "events_obs_n        = events_obs['n']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10422ee3",
   "metadata": {},
   "source": [
    "Note: Keep in mind that this is still a _simulated_ dataset, which means we must still take into account its weights. Of course, in a real LHC dataset, each entry represents exactly one entry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e5855",
   "metadata": {},
   "source": [
    "## 2. Evaluating the likelihood: rate term\n",
    "\n",
    "As already introduced, our likelihood function is composed of two terms, the first of which can be readily evaluated as\n",
    "\n",
    "$$\\mathcal{L}_{\\mathrm{rate}}(\\mu | \\mathcal{D}) = \\mathcal{P}(n ; \\nu(\\mu)) = \\frac{\\nu^{n}(\\mu) e^{-\\nu(\\mu)}}{n!},$$\n",
    "\n",
    "where $n$ is the total number of events in the observed dataset, and $\\nu(\\mu)$ is the expected number of events, which (of course) depends on the $\\mu$-hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1f613",
   "metadata": {},
   "source": [
    "### 2.(b) Compute the total expected number of events, $\\nu(\\mu = 1)$\n",
    "\n",
    "The expected number of events is given by the cross section times luminosity:\n",
    "\n",
    "$$\\nu(\\mu) = \\sigma(\\mu) \\times L$$\n",
    "\n",
    "where the cross section, of course, depends on the POI. These correspond to \"visual\" cross sections, i.e. after detector acceptance/efficiency effects, of the total $gg\\to(h^{\\ast}\\to)ZZ\\to 4\\ell$ process. The independent signal/background/interference contributions to the signal+background+interference hypothesis is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "295465d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sm/ggzz4l_xs.json', 'r') as f:\n",
    "    xs = json.load(f)  # fb\n",
    "    xs_sig_sm = np.prod(xs['sig'])\n",
    "    xs_bkg_sm = np.prod(xs['bkg'])\n",
    "    xs_int_sm = np.prod(xs['int'])\n",
    "    xs_sbi_sm = np.prod(xs['sbi'])\n",
    "\n",
    "nu_sig_sm = xs_sig_sm * lumi\n",
    "nu_bkg_sm = xs_bkg_sm * lumi\n",
    "nu_int_sm = xs_int_sm * lumi\n",
    "nu_sbi_sm = xs_sbi_sm * lumi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c31d4",
   "metadata": {},
   "source": [
    "### 2.(c) Compute the total expected number of events as a function of $\\mu$\n",
    "\n",
    "The SM-expected cross section and event yields correspond to the $\\mu = 1$ hypothesis. Now we must compute how this changes as a function of our parameter of interest according to the following:\n",
    "\n",
    "$$\\nu_{\\mathrm{SBI}}(\\mu) = \\mu \\nu_{\\mathrm{S}}(1) + \\sqrt{\\mu} \\nu_{\\mathrm{I}}(1) + \\nu_{\\mathrm{B}}(1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fda5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_sig_mu = nu_sig_sm * mu_space\n",
    "nu_int_mu = nu_int_sm * torch.sqrt(mu_space)\n",
    "nu_bkg_sm = nu_bkg_sm  # I don't change!\n",
    "nu_sbi_mu = nu_sig_mu + nu_int_mu + nu_bkg_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142778d",
   "metadata": {},
   "source": [
    "### 2.(d) Define & compute the Poisson likelihood\n",
    "\n",
    "Using the quantities computed above, we can now compute the negative log likelihood (NLL) of the rate term as a function of $\\mu$:\n",
    "\n",
    "$$- \\log \\mathcal{L}_{\\mathrm{rate}}(\\mu | \\mathcal{D})  = \\nu(\\mu) - n \\log\\nu(\\mu) + \\cancel{\\log(n!)}$$\n",
    "\n",
    "Reminder the \"disappearance\" of $\\log (1/n!)$ term does not affect the minimization of NLL as a function of $\\mu$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6034b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_pois(n_obs, nu_exp):\n",
    "    \"\"\"\n",
    "    Evaluates the negative log-likelihood for a Poisson process.\n",
    "    \"\"\"\n",
    "    return nu_exp - n_obs * torch.log(nu_exp)\n",
    "\n",
    "# IMPLEMENT ME\n",
    "t_rate = neg_log_pois(events_obs_n.sum(), nu_sbi_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48038035",
   "metadata": {},
   "source": [
    "Now let's plot our rate-only analysis result. As mentioned previous about the arbitrary minimum value of the NLL, we set it to be the zero-point of the test statistic:\n",
    "\n",
    "$$ t \\equiv - 2 \\log \\lambda = -2 \\log (\\frac{\\mathcal{L}(\\mu | \\mathcal{D})}{\\min \\mathcal{L}(\\mu | \\mathcal{D})})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffb61852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nll(mu_space, t):\n",
    "    \n",
    "    t_min = torch.min(t)\n",
    "    t -= t_min\n",
    "\n",
    "    mu_fit = mu_space[torch.argmin(t)]\n",
    "\n",
    "    plt.plot(mu_space.cpu(), t.cpu())\n",
    "    plt.xlim(0, 4)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.scatter(mu_fit.cpu(), 0.0, color='red', label=f'$\\hat{{\\mu}} = {mu_fit:.2f}$')\n",
    "\n",
    "    plt.hlines(1.0, 0, 4,color='gray', linestyle='--', label='$1\\sigma$')\n",
    "    plt.hlines(4.0, 0, 4,color='gray', linestyle='--', label='$2\\sigma$')\n",
    "\n",
    "    plt.xlabel('$\\mu$')\n",
    "    plt.ylabel('$-2 \\log \\lambda$')\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_nll(mu_space, t_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072594cc",
   "metadata": {},
   "source": [
    "Do you notice anything strange about the NLL curve? Perhaps more than one possible minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505c900",
   "metadata": {},
   "source": [
    "## 3. Evaluating the likelihood (ratio): shape term\n",
    "\n",
    "Here comes NSBI, which will estimate the shape term of our likelihood, and (hopefully) improve our results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada18999",
   "metadata": {},
   "source": [
    "### 3.(a) Load the NN models\n",
    "\n",
    "Let's first load the CARL models that we've trained in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf897c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = 'run/h4l'\n",
    "# we ignore the first two objects, which are the picked training & validation datasets\n",
    "scaler_sbi_over_bkg, model_sbi_over_bkg = carl.utils.load_results(run_dir, 'sbi_over_bkg')\n",
    "scaler_sig_over_bkg, model_sig_over_bkg = carl.utils.load_results(run_dir, 'sig_over_bkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2626562",
   "metadata": {},
   "source": [
    "### 3.(b) Run the models over the dataset, and perform the likelihood ratio trick\n",
    "\n",
    "This part should also be straightforward, given the previous exercise:\n",
    "\n",
    "2. Scale the features of the observed data using the scaler from training.\n",
    "3. Run the model over the scaled features.\n",
    "4. Perform the likelihood trick over the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cc88df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sig_over_bkg_sm = scaler_sig_over_bkg.transform(events_obs_features.to_numpy())\n",
    "X_sbi_over_bkg_sm = scaler_sbi_over_bkg.transform(events_obs_features.to_numpy())\n",
    "\n",
    "dl_sig_over_bkg_sm = DataLoader(TensorDataset(torch.tensor(X_sig_over_bkg_sm, dtype=torch.float32)), batch_size=1024) \n",
    "dl_sbi_over_bkg_sm = DataLoader(TensorDataset(torch.tensor(X_sbi_over_bkg_sm, dtype=torch.float32)), batch_size=1024) \n",
    "\n",
    "trainer = L.Trainer(accelerator='gpu', devices=1)\n",
    "s_sig_over_bkg_sm = torch.cat(trainer.predict(model_sig_over_bkg, dl_sig_over_bkg_sm))\n",
    "s_sbi_over_bkg_sm = torch.cat(trainer.predict(model_sbi_over_bkg, dl_sbi_over_bkg_sm))\n",
    "\n",
    "r_sig_over_bkg_sm = s_sig_over_bkg_sm / (1 - s_sig_over_bkg_sm)\n",
    "r_sbi_over_bkg_sm = s_sbi_over_bkg_sm / (1 - s_sbi_over_bkg_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce176da",
   "metadata": {},
   "source": [
    "### 3.(c) Evaluate the probability density ratio\n",
    "\n",
    "Armed with the NSBI estiamte of each term in the probability mixture model, we can now evaluate the full SBI probability (ratio) as:\n",
    "\n",
    "$$\n",
    "\\frac{p_{\\rm SBI} (x | \\mu)}{p_{\\rm B} (x)} = \\frac{ (\\mu - \\sqrt{\\mu}) \\sigma_{\\rm S} r_{\\rm S} (x) + \\sqrt{\\mu} \\sigma_{\\rm SBI} r_{\\rm SBI} (x) + (1-\\sqrt{\\mu}) \\sigma_{\\rm B} }{ \\mu \\sigma_{\\rm S} + \\sqrt{\\mu} \\sigma_{\\rm I} + \\sigma_{\\rm B} }\n",
    "$$\n",
    "\n",
    "Tip: if you want to compute all elements of the $N \\times M$ tensor where $N$ is the number of entries in the dataset and $M$ is the number of $\\mu$ values being tested, then you can utilize tensor broadcasting, here's an example:\n",
    "```py\n",
    "a.shape  # (N,)\n",
    "b.shape  # (M,)\n",
    "c = a[:,None] * b[None,:]\n",
    "c.shape  # (N, M)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7f5d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_sig = mu_space - torch.sqrt(mu_space)\n",
    "multiplier_sbi = torch.sqrt(mu_space)\n",
    "multiplier_bkg = 1 - torch.sqrt(mu_space)\n",
    "\n",
    "# IMPLEMENT ME\n",
    "r_sbi_over_bkg_mu = ( xs_sig_sm * multiplier_sig[None,:] * r_sig_over_bkg_sm[:,None] + xs_sbi_sm * multiplier_sbi[None,:] * r_sbi_over_bkg_sm[:,None] + xs_bkg_sm * multiplier_bkg[None,:] ) / (xs_sig_sm * mu_space + xs_int_sm * torch.sqrt(mu_space) + xs_bkg_sm)\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5678fad",
   "metadata": {},
   "source": [
    "All that's left to do now is to sum over the negative log:\n",
    "\n",
    "$$t_{\\rm shape} = - \\sum_{i=1}^{n} \\left( \\log \\frac{p_{\\rm SBI} (x | \\mu)}{p_{\\rm B} (x)} \\right).$$\n",
    "\n",
    "Reminder: we are dealing with a simulated dataset, with event weights! So the sum over $i=1,\\dots, n$ must be adjusted as\n",
    "\n",
    "$$\n",
    "t_{\\rm shape} = - \\sum_{i=1}^{m} \\left( \\log \\frac{p_{\\rm SBI} (x | \\mu)}{p_{\\rm B} (x)} \\right)^{w_i}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e7027de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT ME\n",
    "t_shape = -2 * torch.sum(torch.tensor(events_obs_n)[:,None] * torch.log(r_sbi_over_bkg_mu), dim=0)\n",
    "# ---\n",
    "plot_nll(mu_space, t_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02dfc94",
   "metadata": {},
   "source": [
    "# Evaluating the likelihood (ratio): rate + shape\n",
    "\n",
    "Once we know both the rate & shape terms, the combined NLL is simply obtained by adding them:\n",
    "\n",
    "$$ t = t_{\\rm rate} + t_{\\rm shape} $$\n",
    "\n",
    "As you should have seen already, the rate term is almost a negligible contribution to setting confidence intervals on $\\mu$ in this case. This is partially due to the power of NSBI, which enables a fully-differential (per-event) shape analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72a314d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t_rate + t_shape\n",
    "plot_nll(mu_space, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
