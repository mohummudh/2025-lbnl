{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e04280",
   "metadata": {},
   "source": [
    "# NSBI training\n",
    "\n",
    "In this chapter, you will be training surrogate neural networks that estimate the ratio of probability densities of an event under different hypotheses, referred to as CARL models.\n",
    "\n",
    "$$\n",
    "r( x | \\theta_1, \\theta_2 ) \\equiv \\frac{p(x | \\theta_1)}{p( x | \\theta_2)}\n",
    "$$\n",
    "\n",
    "We will be learning the density ratio of two numerator hypotheses with respect to a common denominator: (1) the signal-only and (2) signal+background+interference processes over the background-only process.\n",
    "\n",
    "$$\n",
    "r( x | {\\rm S}, {\\rm B}), \\, r( x | {\\rm SBI}, {\\rm B})\n",
    "$$\n",
    "\n",
    "Re-refer to the introductory overview and as to how these estimates allow us to obtain an estimate of the full SBI process under modifications to the Higgs signal strength, which we will fully realize later on in the last chapter. For now, we focus on the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf547bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import lightning as L\n",
    "\n",
    "from physics.simulation import mcfm\n",
    "from physics.analysis import zz4l, zz2l2v\n",
    "from physics.hstar import sigstr\n",
    "from nsbi import carl\n",
    "\n",
    "import matplotlib, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1eb0fe",
   "metadata": {},
   "source": [
    "## 1. Preparing the training datasets\n",
    "\n",
    "The training data consists of examples drawn from the two hypotheses of which we wish to estimate the ratio of. They will correspondingly be referred to as the numerator and denominator hypotheses from this point on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/global/cfs/cdirs/trn016/carl_models/'\n",
    "\n",
    "(events_sig_train, events_sig_val), (events_bkg_train, events_bkg_val) = carl.utils.load_data(data_dir, 'sig_over_bkg')\n",
    "(events_sbi_train, events_sbi_val), _ = carl.utils.load_data(data_dir, 'sbi_over_bkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd005e5b",
   "metadata": {},
   "source": [
    "### 1.(a) Scale the features\n",
    "\n",
    "The first \"usual\" thing to do is to scale the features to have $\\left< x \\right> = 0$ and standard deviation $\\sigma_x = 1$, referred to as standard scaling. Perform the following:\n",
    "\n",
    "1. Scale the features of the *training* data such that the above holds exactly.\n",
    "2. Scale the features of *validation* data exactly according to the scaling performed to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features_4l = ['l1_pt', 'l1_eta', 'l1_phi', 'l1_energy', 'l2_pt', 'l2_eta', 'l2_phi', 'l2_energy', 'l3_pt', 'l3_eta', 'l3_phi', 'l3_energy', 'l4_pt', 'l4_eta', 'l4_phi', 'l4_energy']\n",
    "\n",
    "X_sig_train = scaler.fit_transform(events_sig_train.kinematics[features_4l].to_numpy())\n",
    "X_sig_val = scaler.transform(events_sig_val.kinematics[features_4l].to_numpy())\n",
    "\n",
    "X_sbi_train = scaler.fit_transform(events_sbi_train.kinematics[features_4l].to_numpy())\n",
    "X_sbi_val = scaler.transform(events_sbi_val.kinematics[features_4l].to_numpy())\n",
    "\n",
    "X_bkg_train = scaler.fit_transform(events_bkg_train.kinematics[features_4l].to_numpy())\n",
    "X_bkg_val = scaler.transform(events_bkg_val.kinematics[features_4l].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d21de",
   "metadata": {},
   "source": [
    "### 1.(b) \"Balance\" the hypotheses\n",
    "\n",
    "The key to the likelihood ratio trick is to ensure that the neural network sees examples of two hypotheses that are balanced, i.e. their total rate of occurences in the training data are the same.\n",
    "\n",
    "$$\n",
    "    N(y = 0) = N(y = 1)\n",
    "$$\n",
    "\n",
    "For $N = 1$, then of course the neural networks only sees the relative rate, i.e. probability, of each event under the different hypotheses. Let's enforce these for each of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9364271",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sig_train, w_sig_val = events_sig_train.weights / events_sig_train.weights.sum(), events_sig_val.weights / events_sig_val.weights.sum()\n",
    "w_sbi_train, w_sbi_val = events_sbi_train.weights / events_sbi_train.weights.sum(), events_sbi_val.weights / events_sbi_val.weights.sum()\n",
    "w_bkg_train, w_bkg_val = events_bkg_train.weights / events_bkg_train.weights.sum(), events_bkg_val.weights / events_bkg_val.weights.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67452be",
   "metadata": {},
   "source": [
    "## 2. Building the NN\n",
    "\n",
    "### 2.(a) NN architecture\n",
    "\n",
    "Implement the function to specify the layers of a multi-layer perceptron (MLP) with:\n",
    "\n",
    "1. As many input nodes as there are features,\n",
    "2. As many hidden layer-times-nodes as desired, all with a ReLU activation function.\n",
    "4. One output node with a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e4063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def nn_layers(n_features, n_layers, n_nodes):\n",
    "    layers = []\n",
    "    layers.append(nn.Sequential(nn.Linear(n_features, n_nodes), nn.ReLU()))\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(nn.Sequential(nn.Linear(n_nodes, n_nodes), nn.ReLU()))\n",
    "    layers.append(nn.Sequential(nn.Linear(n_nodes, 1), nn.Sigmoid()))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debcdf4b",
   "metadata": {},
   "source": [
    "## 3. Training the NNs\n",
    "\n",
    "A `torch/lightning` implementation of everything above has already been prepared for you, and can be launched with a command such as:\n",
    "\n",
    "```sh\n",
    " python -m nsbi.carl fit \\\n",
    "    --data.features '[\"l1_pt\", \"l1_eta\", \"l1_phi\", \"l1_energy\", \"l2_pt\", \"l2_eta\", \"l2_phi\", \"l2_energy\", \"l3_pt\", \"l3_eta\", \"l3_phi\", \"l3_energy\", \"l4_pt\", \"l4_eta\", \"l4_phi\", \"l4_energy\"]' \\\n",
    "    --data.numerator_events '/ptmp/mpp/taepa/higgs-offshell-interpretation/data/zz4l/ggZZ_sbi/analyzed.csv' \\\n",
    "    --data.denominator_events '/ptmp/mpp/taepa/higgs-offshell-interpretation/data/zz4l/ggZZ_sbi/analyzed.csv' \\\n",
    "    --data.denominator_reweight '[\"sbi\",\"bkg\"]' \\\n",
    "    --data.batch_size BATCH_SIZE \\\n",
    "    --model.learning_rate LEARNING_RATE \\\n",
    "    --model.n_layers N_LAYERS \\\n",
    "    --model.n_nodes N_NODES \\\n",
    "    --trainer.max_epochs 500 \\\n",
    "    --trainer.seed_everything SEED\n",
    "```\n",
    "\n",
    "The example commands for training the two estimates, $p_{\\rm S (x)} / p_{\\rm B}(x)$ and $p_{\\rm SBI}(x) / p_{\\rm B}(x)$, along with some suggested hyperparameters are already available for you at `scripts/fit-carl-4l.sh`. IMPORTANT: It is *strongly* encouraged (for convenience) to also additionally specify the seed as:\n",
    "```sh\n",
    "    --trainer.seed_everything SEED\n",
    "```\n",
    "This ensures that the train/validation/test dataset splitting is done consistently across the two NNs. This is not required, but convenient (i.e. you will need to write additional code otherwise) for later chapters!\n",
    "\n",
    "### Extra note about \"reweighting\"\n",
    "\n",
    "You may have noticed that for the $p_{\\rm SBI}(x) / p_{\\rm B}(x)$ training, that the SBI dataset is specified as both the SBI and B hypotheses! But also notice the extra argument `--data.denominator_reweight '[\"sbi\", \"bkg\"]'`, which internally reweights the SBI event weights to the B hypothesis, via\n",
    "\n",
    "$$\n",
    " w_{i, \\rm SBI \\to B} = w_{i, SBI} \\times \\frac{|\\mathcal{M}_{\\rm SBI}|^2}{|\\mathcal{M}_{\\rm B}|^2}\n",
    "$$\n",
    "\n",
    "This helps the training converge faster given the available dataset size as examples of the exact same $x_i$ points are seen by the network under both $y_i = 0, 1$ labels. This is actually a prelude to some of the more advanced NSBI methods ([arXiv.1805.12244](https://arxiv.org/abs/1805.12244)), which are beyond the scope of this tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4FP2025_Day2",
   "language": "python",
   "name": "training_env_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
