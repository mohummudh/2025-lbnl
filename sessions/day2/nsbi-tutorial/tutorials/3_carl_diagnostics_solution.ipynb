{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fe27ba",
   "metadata": {},
   "source": [
    "# NSBI diagnostics\n",
    "\n",
    "In the previous chapter, we've trained CARL models that approximate the density ratio between two hypotheses of an event:\n",
    "\n",
    "$$\n",
    " \\hat{r}(x ; H_1, H_2) \\sim r ( x ; H_1, H_2) \\equiv \\frac{p(x | H_1)}{p(x | H_2)}\n",
    "$$\n",
    "\n",
    "The validity of our NSBI results, of course, depends on close this approixmation, $~$, holds. We will perform two diagnostics to gauge how well our estimates are performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529af8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "import vector\n",
    "import hist\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightning as L\n",
    "\n",
    "from physics.analysis import zz4l, zz2l2v\n",
    "from datasets.balanced import BalancedDataset\n",
    "from nsbi import carl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8baf229",
   "metadata": {},
   "source": [
    "## 0. Load the training/validation data, scaler, and model checkpoint\n",
    "\n",
    "First, we'll load everything we need. Note that if you set the same seed for the two models come into play, you only need to load denominator hypothesis datasets once; otherwise you will need the denominator datasets corresponding to each model separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917aeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = '/global/cfs/cdirs/trn016/carl_models'\n",
    "\n",
    "(events_sig_train, events_sig_val), (events_bkg_train, events_bkg_val) = carl.utils.load_data(run_dir, 'sig_over_bkg')\n",
    "(events_sbi_train, events_sbi_val), _ = carl.utils.load_data(run_dir, 'sbi_over_bkg')  # same seed!\n",
    "\n",
    "scaler_sig_over_bkg, model_sig_over_bkg = carl.utils.load_results(run_dir, 'sig_over_bkg')\n",
    "scaler_sbi_over_bkg, model_sbi_over_bkg = carl.utils.load_results(run_dir, 'sbi_over_bkg')\n",
    "\n",
    "features_4l = ['l1_pt', 'l1_eta', 'l1_phi', 'l1_energy', 'l2_pt', 'l2_eta', 'l2_phi', 'l2_energy', 'l3_pt', 'l3_eta', 'l3_phi', 'l3_energy', 'l4_pt', 'l4_eta', 'l4_phi', 'l4_energy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffb651-8cec-44ed-8bed-c73d4d21c393",
   "metadata": {},
   "source": [
    "## 1. Expectation value\n",
    "\n",
    "A fundamental property that a probability distribution function must satisfy is that its integral is $1$. In other words, the expectation value of any DRE over the denominator hypothesis must be that:\n",
    "\n",
    "$$\n",
    "\\left< r(x ; N, D) \\right>_{D} = \\int dx \\; \\frac{p(x | N)}{\\cancel{p(x | D)}} \\; \\cancel{p(x | D)} = 1.\n",
    "$$\n",
    "\n",
    "Let's explicitly verify this for each of the two DREs, $\\rm S/B$ and $\\rm SBI/B$, using the training data:\n",
    "\n",
    "1. Scale the background events' features, $x$.\n",
    "2. Obtain the classifier decisions, $\\hat s(x)$, over the events.\n",
    "3. Perform the likelihood trick over the output, $\\hat r = \\hat s/(1-\\hat s)$.\n",
    "4. Compute $C = \\left< r \\right>^{-1}_{\\rm B}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293dbc8-7024-4361-8870-5fdf5a80bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration_factor(dre, scaler, events_denom):\n",
    "    \"\"\"\n",
    "    Evaluate the calibration factor of a DRE, i.e. inverse of its expectation value over denominator hypothesis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dre : torch.nn.Module\n",
    "        Trained DRE model (e.g. a CARL instance loaded from a checkpoint).\n",
    "    events_denom : array-like or torch.Tensor\n",
    "        Input events sampled from the denominator hypothesis distribution.\n",
    "    scaler : sklearn.preprocessing.StandardScaler or similar\n",
    "        Fitted scaler used to normalize the input features before passing them to the model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Model predictions for the density ratio evaluated on the input events.\n",
    "    \"\"\"\n",
    "    # compute denominator hypothesis density\n",
    "    p_denom = torch.tensor(events_denom.weights / events_denom.weights.sum())\n",
    "\n",
    "    # run model over (scaled) denominator event inputs\n",
    "    trainer = L.Trainer(accelerator='gpu', devices=1)    \n",
    "    X_denom = scaler.transform(events_denom.kinematics[features_4l].to_numpy())\n",
    "    dl_denom = DataLoader(TensorDataset(torch.tensor(X_denom, dtype=torch.float32)), batch_size=1024)\n",
    "    s_num_vs_denom = torch.cat(trainer.predict(dre, dl_denom))\n",
    "\n",
    "    # IMPLEMENT ME\n",
    "    # likelihood ratio trick\n",
    "   add code here\n",
    "    \n",
    "    # inverse(!) of expectation value\n",
    "    C_dre = 1.0/torch.sum(r_num_over_denom * p_denom)\n",
    "    # ---\n",
    "\n",
    "    return C_dre.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca98d19-d574-4826-aaf1-ebcf60bbf1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_sig_over_bkg = calibration_factor(model_sig_over_bkg, scaler_sig_over_bkg, events_bkg_train)\n",
    "C_sbi_over_bkg = calibration_factor(model_sbi_over_bkg, scaler_sbi_over_bkg, events_bkg_train)\n",
    "print('C(S/B) = ', C_sig_over_bkg)\n",
    "print('C(SBI/B) = ', C_sbi_over_bkg)\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3771e-7962-4cd5-9531-d33341e3ab78",
   "metadata": {},
   "source": [
    "As you might already have guessed, the inverse of the expectation value can be used as a *calibration factor* for the DRE as:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{\\rm calib}(x) = C \\times \\hat{r} (x)\n",
    "$$\n",
    "\n",
    "So we should keep those numbers for later use! This procedure will ensure that the DRE is, on average, indeed corresponds to the ratio of probabilities. But note that this will not improve the accuracy of the DRE on a per-event basis, i.e. its variance.\n",
    "\n",
    "This process can be considered to be still within \"training\" of the NN in the sense that (1) its purpose to improve its accuracy, and (2) we cannot perform this process over any testing data without any unblinding predjudice. Hence we perform this calibration over the denominator hypothesis events drawn from training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331bfd60",
   "metadata": {},
   "source": [
    "## 2. Reweighting\n",
    "\n",
    "Another intuitive expectation of a DRE over the denominator hypothesis, this time on a per-event basis, is that it should provide a reweighting of the probability between hypotheses:\n",
    "\n",
    "$$\n",
    " \\hat{p}(x | H_1) = p(x| H_2) \\times \\hat{r}(x ; H_1, H_2) \\Rightarrow \\hat{w}_{H_1}(x) = w_{H_2}(x) \\times r( x ; H_1, H_2) / N\n",
    "$$\n",
    "\n",
    "where $N$ is an arbitrary normalization factor (remember: we are performing a *density* ratio estimate!).\n",
    "This time, let's perform this check using the *validation* data, we want to \"evaluate\" the performance of the NN independently of the training.\n",
    "\n",
    "1. Evaluate the DRE over the denominator hypothesis event features following a similar procedure as above.\n",
    "2. Multiply this over the denominator hypothesis event weights to estimate the numerator hypothesis weights.\n",
    "3. Plot the original denominator hypothesis event distribution w.r.t. the $m_{4\\ell}$ observable.\n",
    "4. Plot the reweighted denominator-to-numerator hypothesis event distribution.\n",
    "    - How significant of a shape change is this compared to #3?\n",
    "5. Plot the distribution obtained directly from the numerator hypothesis.\n",
    "    - Is the distribution #4 compatible, at least visually, with the correct answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweighting_diagnostic(model, scaler, events_num_val, events_denom_val):\n",
    "    # scale features\n",
    "    X_denom_val = scaler.transform(events_denom_val.kinematics[features_4l].to_numpy())\n",
    "    dl_denom_val = DataLoader(TensorDataset(torch.tensor(X_denom_val, dtype=torch.float32)), batch_size=1024) \n",
    "    \n",
    "    # likelihood ratio trick\n",
    "    trainer = L.Trainer(accelerator='gpu', devices=1)\n",
    "    s_num_vs_denom = torch.cat(trainer.predict(model, dl_denom_val))\n",
    "    r_num_over_denom = s_num_vs_denom / (1 - s_num_vs_denom)\n",
    "    \n",
    "    # histogram definition\n",
    "    m4l_axis = hist.axis.Regular(41, 180, 1000, label = 'm4l')\n",
    "    h_m4l_num = hist.Hist(m4l_axis)\n",
    "    h_m4l_denom = hist.Hist(m4l_axis, label='$D$')\n",
    "    h_m4l_num_from_denom = hist.Hist(m4l_axis, label='$D\\\\rightarrow N$')\n",
    "    \n",
    "    # observable calculation\n",
    "    def calculate_m4l(kinematics):\n",
    "        p_l1 = vector.array({'pt': kinematics['l1_pt'], 'eta': kinematics['l1_eta'], 'phi': kinematics['l1_phi'], 'energy': kinematics['l1_energy']})\n",
    "        p_l2 = vector.array({'pt': kinematics['l2_pt'], 'eta': kinematics['l2_eta'], 'phi': kinematics['l2_phi'], 'energy': kinematics['l2_energy']})\n",
    "        p_l3 = vector.array({'pt': kinematics['l3_pt'], 'eta': kinematics['l3_eta'], 'phi': kinematics['l3_phi'], 'energy': kinematics['l3_energy']})\n",
    "        p_l4 = vector.array({'pt': kinematics['l4_pt'], 'eta': kinematics['l4_eta'], 'phi': kinematics['l4_phi'], 'energy': kinematics['l4_energy']})\n",
    "        return (p_l1 + p_l2 + p_l3 + p_l4).mass\n",
    "    \n",
    "    # weights\n",
    "    w_denom = torch.tensor(events_denom_val.weights)\n",
    "    w_num = torch.tensor(events_num_val.weights)\n",
    "    \n",
    "    w_num_from_denom = torch.tensor(events_denom_val.weights) * r_num_over_denom\n",
    "    # ---\n",
    "    \n",
    "    # normalize to weights probablity\n",
    "    p_denom = w_denom / torch.sum(w_denom)\n",
    "    p_num = w_num / torch.sum(w_num)\n",
    "    p_num_from_denom = w_num_from_denom / torch.sum(w_num_from_denom)\n",
    "    \n",
    "    # populate each histogram with correct (obs, w)\n",
    "    h_m4l_denom.fill( calculate_m4l(events_denom_val.kinematics), weight=p_denom)\n",
    "    h_m4l_num.fill( calculate_m4l(events_num_val.kinematics), weight=p_num)\n",
    "    \n",
    "    h_m4l_num_from_denom.fill( calculate_m4l(events_denom_val.kinematics), weight=p_num_from_denom)\n",
    "    # ---\n",
    "    \n",
    "    # plotting\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    plt.stairs(h_m4l_denom.values(), h_m4l_denom.axes[0].edges, label='$D$')\n",
    "    plt.stairs(h_m4l_num.values(), h_m4l_num.axes[0].edges, label='$N$')\n",
    "    plt.stairs(h_m4l_num_from_denom.values(), h_m4l_num_from_denom.axes[0].edges, label='$\\\\frac{N}{D} \\\\times D$')\n",
    "    plt.legend()\n",
    "    \n",
    "reweighting_diagnostic(model_sbi_over_bkg, scaler_sbi_over_bkg, events_sbi_val, events_bkg_val)\n",
    "reweighting_diagnostic(model_sig_over_bkg, scaler_sig_over_bkg, events_sig_val, events_bkg_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab87a64",
   "metadata": {},
   "source": [
    "As performed in Chapter 1, you are encouraged to check in more detail via ratio plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5b149",
   "metadata": {},
   "source": [
    "## 3. Calibration curve\n",
    "\n",
    "In section 2, we've checked qualitatively the accuracy of our DRE w.r.t. *one* projection of our observable space, $m_{4\\ell}$. \n",
    "In principle, we could obtain an improved calibration function, e.g. $C(m_{4\\ell})$.\n",
    "But what about the other 15 variables? This is probably not feasible without training data that is several orders of magnitude larger...\n",
    "\n",
    "There is an alternative, feasible, and more systematic way to assess the per-event accuracy of our DREs.\n",
    "First, consider examples drawn from the numerator & denominator hypothesis, drawn in a balanced way & labelled $y_i = 0,1$ exactly as we did for training our DREs in the first place.\n",
    "Then, running our classifier over this dataset yields the pre-likelihood ratio trick decision:\n",
    "$$\n",
    "    \\hat s( x ; N, D) \\sim \\frac{p_N(x)}{p_N(x) + p_D(x)}\n",
    "$$\n",
    "For each sub-population of events of a specific (or some range of) $\\hat s$ value, there will be a mixture of (again, labelled) events have have been drawn from the numerator or denominator hypotheses, which can be directly counted.\n",
    "Computing the relative yield of the numerator events over the total then yields an \"MC estimate\" of the true probability ratio.\n",
    "This is referred to as the _calibration_ curve of the DRE.\n",
    "\n",
    "\n",
    "as an unbiased function of $x$. We can check if this is satisfied by performing the following:\n",
    "\n",
    "1. Bin events sampled from the numerator & denominator hypotheses according the classifier estimate, $\\hat s$.\n",
    "    - IMPORTANT: Remember to apply the calibration factor, $C$, from above!\n",
    "3. Obtain distributions, i.e. histograms, of the classifier decision weighted by the (balanced) event weights.\n",
    "    - IMPORTANT: One each for numerator & denominator hypothesis events, identified by $y=1,0$!\n",
    "4. Use the histograms to compute the MC estimate in each bin, $s = \\frac{N(y=1)}{N(y=0) + N(y=1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee8411",
   "metadata": {},
   "source": [
    "### 3.(a) Running the model over the numerator & denominator hypothesis datasets\n",
    "\n",
    "We will make use of the pre-available `BalancedDataset` implementation (which was also used in the training) to consistently perform the $x$-scaling, event weight balancing, and $y = 0,1$ labeling of the numerator & denominator hypotheses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a9059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration_curve(model, scaler, events_num, events_denom, calibration_factor):\n",
    "    \"\"\"\n",
    "    Obtain calibration curve, C(x) of a DRE\n",
    "    Parameters\n",
    "    ---\n",
    "    model: DRE model\n",
    "    scaler: feature scaler\n",
    "    events_num: numerator hypothesis events\n",
    "    events_denom: denominator hypothesis events\n",
    "    calibration_factor: overall calibration factor from expectation value\n",
    "    \"\"\"\n",
    "\n",
    "    # classifier decision\n",
    "    ds_balanced = BalancedDataset(events_num, events_denom, features=features_4l, scaler=scaler)\n",
    "    dl = torch.utils.data.DataLoader(torch.tensor(ds_balanced.X, dtype=torch.float32), batch_size=1024)\n",
    "    trainer = L.Trainer(accelerator='gpu', devices=1)\n",
    "    s_num_vs_denom = torch.cat(trainer.predict(model, dl)).detach().numpy()\n",
    "\n",
    "    # likelihood ratio trick -> apply calibration factor -> (lhrt)^(-1)\n",
    "    r_num_over_denom = s_num_vs_denom/(1-s_num_vs_denom)\n",
    "    r_num_over_denom * calibration_factor\n",
    "    s_num_vs_denom = r_num_over_denom / (1+r_num_over_denom)\n",
    "    # ---\n",
    "    \n",
    "    # (balanced) event weight?\n",
    "    w_balanced = ds_balanced.w\n",
    "    \n",
    "    # true label (1 = numerator, 0 = denominator)\n",
    "    y_num_or_denom = ds_balanced.s\n",
    "\n",
    "    # s binning & histograms\n",
    "    s_bins = np.linspace(0.0, 1.0, 41)\n",
    "    s_centers = (s_bins[:-1] + s_bins[1:])/2\n",
    "    s_nbins = len(s_bins) -1\n",
    "    s_widths = (s_bins[1:] - s_bins[:-1])/2\n",
    "    s_axis = hist.axis.Regular(40,0,1.0)\n",
    "    h_num, h_denom = hist.Hist(s_axis, storage=hist.storage.Weight()), hist.Hist(s_axis, storage=hist.storage.Weight())\n",
    "\n",
    "    # histogram using DRE as observable using balanced weights\n",
    "    # select only events with the correct labels for num & denom hypo's!\n",
    "    h_num.fill(s_num_vs_denom[y_num_or_denom==1], weight = w_balanced[y_num_or_denom==1])\n",
    "    h_denom.fill(s_num_vs_denom[y_num_or_denom==0], weight = w_balanced[y_num_or_denom==0])\n",
    "    # ---\n",
    "    \n",
    "    N = h_num.values()\n",
    "    D = h_denom.values()\n",
    "    VN = h_num.variances()\n",
    "    VD = h_denom.variances()\n",
    "    s_mc_val = N / (N + D)\n",
    "    s_mc_err = np.sqrt((D / np.square(N + D))**2 * VN + (N / np.square(N + D))**2 * VD)\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    plt.plot([0,1], [0,1], marker='none', linestyle='--', color='grey')\n",
    "    plt.xlabel(\"NSBI estimate, $\\hat{s}(x)$\")\n",
    "    plt.ylabel(\"MC estimate, $s(x)$\")\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.errorbar(s_centers, s_mc_val, xerr=s_widths, yerr=s_mc_err, linestyle='none')\n",
    "\n",
    "calibration_curve(model_sig_over_bkg, scaler_sig_over_bkg, events_sig_train, events_bkg_train, C_sig_over_bkg)\n",
    "calibration_curve(model_sbi_over_bkg, scaler_sbi_over_bkg, events_sbi_train, events_bkg_train, C_sbi_over_bkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebaca2",
   "metadata": {},
   "source": [
    "- How do the calibration curve look? Is it close to a diagonal line?\n",
    "- What do you notice about the range & variance of the curves between the two DREs? Why do you think such differences come about?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac146f6-09e5-4449-a49a-b4cad7aa67d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.(f) Extra: remarks on \"MC estimate\"\n",
    "\n",
    "The calibration curve is a function of the DRE output values, $\\hat s(x)$. You may wonder: \n",
    "- Why did we have to bother with calculating an \"MC estimate\" of the true density ratio, $s(x)$?\n",
    "- In fact, why did we not simply evaluate the accuracy of $\\hat s(x)$  as a function of $s(x)$?\n",
    "\n",
    "The answer to this question lies in recognizing that $s(x)$, i.e. the density ratio as a function of observables, is an *intractable* quantity as mentioned previously and the motivation for NSBI in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cdb869-5b43-45f4-a8a5-5fec38ac3717",
   "metadata": {},
   "source": [
    "### 3.(g) Extra: from calibration factor to function\n",
    "\n",
    "Extra: The calibration curve is useful in the sense that now we can quantitavely evaluate how (in)accurate our network predictions are as a function of its input variables. Mathematically speaking, as long as the curve obtained above is _monotonic_, one can in principle derive a calibration function, $C(x)$,\n",
    "\n",
    "$$\n",
    "C(x) \\times \\left< \\hat{s}(x) \\right> = \\left< s(x) \\right>\n",
    "$$,\n",
    "\n",
    "where $C(x)$ is now a per-event function, as opposed to a global factor from the expectation value approach! For the next chapter we will assume that this calibration is perfect; the next tutorial on ensembling will allow us to get a handle on additional uncertainties that should be accounted for due to statistical sources including this model mis-specification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4FP2025_Day2",
   "language": "python",
   "name": "training_env_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
